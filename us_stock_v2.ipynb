{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01669db-2e76-4c73-92cd-6d021fe25bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atlas\\AppData\\Local\\Temp\\ipykernel_22172\\2958769572.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  daily_price = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "path = 'C:/Users/atlas/OneDrive/Desktop/usstock/'\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "all_csv_files = glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "# Read all CSV files and concatenate them into a single DataFrame\n",
    "dfs = []\n",
    "for csv_file in all_csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "daily_price = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b7a9bc-7d2f-49e0-a0b4-df8ecb5b4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_price = daily_price[['Date','Close','Ticker']]\n",
    "daily_price = daily_price.rename(columns={'Date':'time','Close':'close','Ticker':'ticker'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ce5ab5-b42d-4026-ad81-896b0e29ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>close</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-11-18</td>\n",
       "      <td>31.473534</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-11-19</td>\n",
       "      <td>28.880545</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-11-22</td>\n",
       "      <td>31.473534</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-11-23</td>\n",
       "      <td>28.612303</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-11-24</td>\n",
       "      <td>29.372318</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time      close ticker\n",
       "0  1999-11-18  31.473534      A\n",
       "1  1999-11-19  28.880545      A\n",
       "2  1999-11-22  31.473534      A\n",
       "3  1999-11-23  28.612303      A\n",
       "4  1999-11-24  29.372318      A"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669356ec-7740-4ce7-b8f1-a6f8c8966f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping threshold date: 1962-12-31 because maximum date is NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vnstock import *\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "\n",
    "def yearly_price(df):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['year'] = df['time'].dt.year \n",
    "    df = df.loc[df.groupby(['ticker', 'year'])['time'].idxmax()]\n",
    "    df = df[['time', 'ticker', 'close','year']]  \n",
    "    df = df.copy()\n",
    "    df['next year price'] = df.groupby('ticker')['close'].shift(-1)\n",
    "    df['return'] = df['next year price'] / df['close'] - 1\n",
    "    df['next date'] = df.groupby('ticker')['time'].shift(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def ranking(df):\n",
    "    df['ranking'] = df.groupby('year')['return'].transform(lambda x: x.rank(ascending=False))\n",
    "    df = pd.pivot_table(data=df, values = \"ranking\", index = \"ticker\", columns = \"time\", aggfunc = 'sum', fill_value = 0)\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df = df.T\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.rename(columns={'ticker': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['time'] = df['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def markov(df):\n",
    "    unique_dates = df['time'].unique()\n",
    "    #unique_dates = ['2023-12-31']\n",
    "    final_result_append = []\n",
    "    \n",
    "    for threshold_date in unique_dates:\n",
    "        \n",
    "        df_copy = df.copy()   \n",
    "        df_copy.rename(columns={'ticker': 'time'}, inplace=True)\n",
    "        df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
    "        df_copy['time'] = df_copy['time'].dt.strftime('%Y-%m-%d')\n",
    "        threshold_date_timestamp = pd.Timestamp(threshold_date)\n",
    "        \n",
    "        # Filter the DataFrame to include only dates before the current threshold date\n",
    "        con2 = df_copy['time'] < threshold_date_timestamp.strftime('%Y-%m-%d')\n",
    "        df_filtered = df_copy[con2]\n",
    "        df_filtered = df_filtered.sort_values(by='time')\n",
    "    \n",
    "        # Create an empty list to store the results\n",
    "        results = []\n",
    "    \n",
    "        # Loop over each column after the second column (as the first one is 'time')\n",
    "        for col_name in df_filtered.columns[1:]:\n",
    "            second_column = df_filtered[col_name]\n",
    "    \n",
    "            # Manually input 400 rankings\n",
    "            rankings = [i for i in range(1, 401)]\n",
    "    \n",
    "            # Calculate transition matrix using the second column\n",
    "            transition_matrix = pd.crosstab(second_column.shift(), second_column, normalize='index')\n",
    "    \n",
    "            # Reindex transition matrix to include all possible rankings\n",
    "            transition_matrix = transition_matrix.reindex(index=rankings, columns=rankings, fill_value=0)\n",
    "    \n",
    "            # Check if the transition matrix satisfies Markov Chain assumptions\n",
    "            # if not all(transition_matrix.sum(axis=1).round(8) == 1):\n",
    "                #print(\"Transition matrix does not satisfy Markov Chain assumptions\")\n",
    "    \n",
    "            # Example transition matrix dimensions (400x400)\n",
    "            matrix_size = 400\n",
    "            # Create a random transition matrix\n",
    "            transition_matrix_data = np.random.rand(matrix_size, matrix_size)\n",
    "            # Normalize each row to ensure that row sums are equal to 1\n",
    "            transition_matrix_data = transition_matrix_data / transition_matrix_data.sum(axis=1, keepdims=True)\n",
    "            # Create DataFrame with random values and rankings as indices and columns\n",
    "            transition_matrix_df = pd.DataFrame(transition_matrix_data, index=range(1, matrix_size + 1),\n",
    "                                                columns=range(1, matrix_size + 1))\n",
    "    \n",
    "            # Example stationary distribution dimensions (400x1)\n",
    "            # Create a random stationary distribution\n",
    "            stationary_distribution_data = np.random.rand(matrix_size, 1)\n",
    "            # Normalize the distribution to ensure that the sum equals 1\n",
    "            stationary_distribution_data = stationary_distribution_data / stationary_distribution_data.sum()\n",
    "            # Create DataFrame with random values and rankings as index\n",
    "            stationary_distribution_df = pd.DataFrame(stationary_distribution_data, index=range(1, matrix_size + 1),\n",
    "                                                      columns=['Probability'])\n",
    "    \n",
    "            # Assuming stationary_distribution_df contains the stationary distribution and transition_matrix_df contains the transition matrix\n",
    "    \n",
    "            stationary_distribution_transposed = stationary_distribution_df.T\n",
    "            predicted_distribution = stationary_distribution_transposed.dot(transition_matrix_df)\n",
    "            predicted_distribution_with_ticker = pd.concat([pd.DataFrame({'Ticker': [col_name]}),\n",
    "                                                            predicted_distribution.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "            # Append the current result to the results list\n",
    "            results.append(predicted_distribution_with_ticker)\n",
    "    \n",
    "        # Concatenate all results into a single DataFrame\n",
    "        final_result = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "        # Convert the 'time' column to datetime format\n",
    "        df_filtered['time'] = pd.to_datetime(df_filtered['time'])\n",
    "    \n",
    "        max_time = df_filtered['time'].max()\n",
    "    \n",
    "        if not pd.isnull(max_time):\n",
    "            final_result['time'] = max_time ### this time is reporting date, not forecast date. Forecast date should be the date after that\n",
    "            final_result_append.append(final_result)\n",
    "        else:\n",
    "            print(\"Skipping threshold date:\", threshold_date, \"because maximum date is NaN\")\n",
    "    \n",
    "    final_result_df = pd.concat(final_result_append, ignore_index=True)\n",
    "\n",
    "    return final_result_df\n",
    "\n",
    "\n",
    "path = 'C:/Users/atlas/OneDrive/Desktop/usstock2/'\n",
    "#yearly price\n",
    "yearly_price = yearly_price(daily_price)\n",
    "yearly_price.to_csv(path + '/yearly_price.csv', index=False)\n",
    "\n",
    "#raking\n",
    "ranking = ranking(yearly_price)\n",
    "ranking.to_csv(path + '/ranking.csv', index=False)\n",
    "\n",
    "\n",
    "#markov\n",
    "df = markov(ranking)\n",
    "df.to_csv(path + '/df.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
