{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "821c8928-8d95-4a4f-ad65-3f3694478fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vnstock import *\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b911cb9e-1652-4879-b5d7-9dcf2339b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yearly price table\n",
    "path = 'C:/Users/atlas/OneDrive/Desktop/vnstock'\n",
    "df = pd.read_csv(path + '/daily_price.csv')\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year \n",
    "df = df.loc[df.groupby(['ticker', 'year'])['time'].idxmax()]\n",
    "df = df[['time', 'ticker', 'close','year']]  \n",
    "df = df.copy()\n",
    "df['next year price'] = df.groupby('ticker')['close'].shift(-1)\n",
    "df['return'] = df['next year price'] / df['close'] - 1\n",
    "df['next date'] = df.groupby('ticker')['time'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e69acd00-41b9-42dc-9c7f-9c8094ad293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearl_price = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8401693f-8fd5-4d08-82b7-e1542f41b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'time' column to datetime if it's not already\n",
    "yearly_price['time'] = pd.to_datetime(yearly_price['time'])\n",
    "\n",
    "# Function to get the last date of the year for a given date\n",
    "def last_date_of_year(date):\n",
    "    return pd.to_datetime(date.strftime('%Y-12-31'))\n",
    "\n",
    "# Apply the function to 'time' column to get the last date of each year\n",
    "yearly_price_v2 = yearly_price.copy()\n",
    "yearly_price_v2['time'] = yearly_price_v2['time'].apply(last_date_of_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5ff99b7-c66c-45e6-b85c-e89cee78c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = yearly_price_v2['year'] >= 2019\n",
    "con2 = yearly_price_v2['year'] <= 2020\n",
    "\n",
    "input = yearly_price_v2[con * con2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cd5a4-9b46-4bb1-8664-f44df3e2d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa54fd2e-75e0-4bcf-b8b8-3ca259ff968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df['ranking'] = df.groupby('year')['return'].transform(lambda x: x.rank(ascending=False))\n",
    "df = pd.pivot_table(data=df, values=\"ranking\", index=\"ticker\", columns=\"time\", aggfunc='sum', fill_value=0)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.T.reset_index()\n",
    "df.columns = df.iloc[0]\n",
    "df = df.drop(0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.rename(columns={'ticker': 'time'}, inplace=True)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['time'] = df['time'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "194913f2-b082-4959-814e-86746c7f7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92531e94-cd69-45d5-bf24-6d65825abdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2012-12-31', '2013-12-31', '2014-12-31', '2015-12-31',\n",
       "       '2016-12-31', '2017-12-31', '2018-12-31', '2019-12-31',\n",
       "       '2020-12-31'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dates = df['time'].unique()\n",
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c387c32-5e50-4c7c-b03a-2cc00c906dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping threshold date: 2012-12-31 because maximum date is NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_dates = df['time'].unique()\n",
    "#unique_dates = ['2023-12-31']\n",
    "final_result_append = []\n",
    "\n",
    "for threshold_date in unique_dates:\n",
    "    \n",
    "    df_copy = df.copy()   \n",
    "    df_copy.rename(columns={'ticker': 'time'}, inplace=True)\n",
    "    df_copy['time'] = pd.to_datetime(df_copy['time'])\n",
    "    df_copy['time'] = df_copy['time'].dt.strftime('%Y-%m-%d')\n",
    "    threshold_date_timestamp = pd.Timestamp(threshold_date)\n",
    "    \n",
    "    # Filter the DataFrame to include only dates before the current threshold date\n",
    "    con2 = df_copy['time'] < threshold_date_timestamp.strftime('%Y-%m-%d')\n",
    "    df_filtered = df_copy[con2]\n",
    "    df_filtered = df_filtered.sort_values(by='time')\n",
    "\n",
    "    # Create an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Loop over each column after the second column (as the first one is 'time')\n",
    "    for col_name in df_filtered.columns[1:]:\n",
    "        second_column = df_filtered[col_name]\n",
    "\n",
    "        # Manually input 400 rankings\n",
    "        rankings = [i for i in range(1, 401)]\n",
    "\n",
    "        # Calculate transition matrix using the second column\n",
    "        transition_matrix = pd.crosstab(second_column.shift(), second_column, normalize='index')\n",
    "\n",
    "        # Reindex transition matrix to include all possible rankings\n",
    "        transition_matrix = transition_matrix.reindex(index=rankings, columns=rankings, fill_value=0)\n",
    "\n",
    "        # Check if the transition matrix satisfies Markov Chain assumptions\n",
    "        # if not all(transition_matrix.sum(axis=1).round(8) == 1):\n",
    "            #print(\"Transition matrix does not satisfy Markov Chain assumptions\")\n",
    "\n",
    "        # Example transition matrix dimensions (400x400)\n",
    "        matrix_size = 400\n",
    "        # Create a random transition matrix\n",
    "        transition_matrix_data = np.random.rand(matrix_size, matrix_size)\n",
    "        # Normalize each row to ensure that row sums are equal to 1\n",
    "        transition_matrix_data = transition_matrix_data / transition_matrix_data.sum(axis=1, keepdims=True)\n",
    "        # Create DataFrame with random values and rankings as indices and columns\n",
    "        transition_matrix_df = pd.DataFrame(transition_matrix_data, index=range(1, matrix_size + 1),\n",
    "                                            columns=range(1, matrix_size + 1))\n",
    "\n",
    "        # Example stationary distribution dimensions (400x1)\n",
    "        # Create a random stationary distribution\n",
    "        stationary_distribution_data = np.random.rand(matrix_size, 1)\n",
    "        # Normalize the distribution to ensure that the sum equals 1\n",
    "        stationary_distribution_data = stationary_distribution_data / stationary_distribution_data.sum()\n",
    "        # Create DataFrame with random values and rankings as index\n",
    "        stationary_distribution_df = pd.DataFrame(stationary_distribution_data, index=range(1, matrix_size + 1),\n",
    "                                                  columns=['Probability'])\n",
    "\n",
    "        # Assuming stationary_distribution_df contains the stationary distribution and transition_matrix_df contains the transition matrix\n",
    "\n",
    "        stationary_distribution_transposed = stationary_distribution_df.T\n",
    "        predicted_distribution = stationary_distribution_transposed.dot(transition_matrix_df)\n",
    "        predicted_distribution_with_ticker = pd.concat([pd.DataFrame({'Ticker': [col_name]}),\n",
    "                                                        predicted_distribution.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Append the current result to the results list\n",
    "        results.append(predicted_distribution_with_ticker)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_result = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Convert the 'time' column to datetime format\n",
    "    df_filtered['time'] = pd.to_datetime(df_filtered['time'])\n",
    "\n",
    "    max_time = df_filtered['time'].max()\n",
    "\n",
    "    if not pd.isnull(max_time):\n",
    "        final_result['time'] = max_time ### this time is reporting date, not forecast date. Forecast date should be the date after that\n",
    "        final_result_append.append(final_result)\n",
    "    else:\n",
    "        print(\"Skipping threshold date:\", threshold_date, \"because maximum date is NaN\")\n",
    "\n",
    "final_result_df = pd.concat(final_result_append, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
